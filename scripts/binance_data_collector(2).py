import requests
import zipfile
import pandas as pd
from pathlib import Path
import io
from tqdm import tqdm
import time
from datetime import datetime, timedelta
import xml.etree.ElementTree as ET

class BinanceDataCollector:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ Binance —á–µ—Ä–µ–∑ S3 API, —Ä–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ—Ç –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤ –æ–¥–∏–Ω CSV
    """
    
    def __init__(self, symbol, interval='4h'):
        self.symbol = symbol
        self.interval = interval
        self.temp_dir = Path(f'temp_{symbol}')
        self.temp_dir.mkdir(exist_ok=True)
        
        # AWS S3 endpoints –¥–ª—è Binance
        self.s3_base = 'https://s3-ap-northeast-1.amazonaws.com/data.binance.vision'
        
    def get_zip_links(self):
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö .zip —Ñ–∞–π–ª–æ–≤ —á–µ—Ä–µ–∑ S3 API"""
        print(f"\nüìä –ü–æ–ª—É—á–∞—é —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –¥–ª—è {self.symbol}...")
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –∫ S3 API
        prefix = f'data/spot/monthly/klines/{self.symbol}/{self.interval}/'
        url = f'{self.s3_base}?delimiter=/&prefix={prefix}'
        
        try:
            response = requests.get(url, timeout=30)
            response.raise_for_status()
            
            # –ü–∞—Ä—Å–∏–º XML –æ—Ç–≤–µ—Ç –æ—Ç S3
            root = ET.fromstring(response.content)
            
            # Namespace –¥–ª—è S3 XML
            ns = {'s3': 'http://s3.amazonaws.com/doc/2006-03-01/'}
            
            zip_links = []
            
            # –ò—â–µ–º –≤—Å–µ Contents —ç–ª–µ–º–µ–Ω—Ç—ã (—Ñ–∞–π–ª—ã)
            for content in root.findall('s3:Contents', ns):
                key = content.find('s3:Key', ns)
                if key is not None and key.text.endswith('.zip'):
                    file_url = f'{self.s3_base}/{key.text}'
                    zip_links.append(file_url)
            
            # –ï—Å–ª–∏ namespace –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª, –ø—Ä–æ–±—É–µ–º –±–µ–∑ –Ω–µ–≥–æ
            if not zip_links:
                for content in root.findall('Contents'):
                    key = content.find('Key')
                    if key is not None and key.text.endswith('.zip'):
                        file_url = f'{self.s3_base}/{key.text}'
                        zip_links.append(file_url)
            
            print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(zip_links)} –∞—Ä—Ö–∏–≤–æ–≤")
            return sorted(zip_links)
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–ø–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤: {e}")
            print(f"URL: {url}")
            
            # –ó–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç: –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º URLs –ø–æ –∏–∑–≤–µ—Å—Ç–Ω–æ–º—É –ø–∞—Ç—Ç–µ—Ä–Ω—É
            print("\nüîÑ –ü—Ä–æ–±—É—é –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ (–≥–µ–Ω–µ—Ä–∞—Ü–∏—è URLs)...")
            return self.generate_urls_by_pattern()
    
    def generate_urls_by_pattern(self):
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç URLs –ø–æ –∏–∑–≤–µ—Å—Ç–Ω–æ–º—É –ø–∞—Ç—Ç–µ—Ä–Ω—É Binance:
        SYMBOL-INTERVAL-YEAR-MONTH.zip
        """
        zip_links = []
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç
        # BTC —Ç–æ—Ä–≥—É–µ—Ç—Å—è —Å 2017, ETH –ø—Ä–∏–º–µ—Ä–Ω–æ —Å 2017-2018
        start_year = 2017 if self.symbol == 'BTCUSDT' else 2018
        end_date = datetime.now()
        
        current = datetime(start_year, 1, 1)
        
        while current <= end_date:
            year = current.year
            month = str(current.month).zfill(2)
            
            filename = f'{self.symbol}-{self.interval}-{year}-{month}.zip'
            url = f'{self.s3_base}/data/spot/monthly/klines/{self.symbol}/{self.interval}/{filename}'
            
            zip_links.append(url)
            
            # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–º—É –º–µ—Å—è—Ü—É
            if current.month == 12:
                current = datetime(current.year + 1, 1, 1)
            else:
                current = datetime(current.year, current.month + 1, 1)
        
        print(f"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(zip_links)} –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö URLs")
        return zip_links
    
    def download_and_extract(self, url, retries=3):
        """–°–∫–∞—á–∏–≤–∞–µ—Ç –∏ —Ä–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ—Ç –æ–¥–∏–Ω –∞—Ä—Ö–∏–≤ —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
        for attempt in range(retries):
            try:
                # –°–∫–∞—á–∏–≤–∞–µ–º –∞—Ä—Ö–∏–≤ –≤ –ø–∞–º—è—Ç—å
                response = requests.get(url, timeout=60)
                
                # –ï—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω (404), –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                if response.status_code == 404:
                    return None
                    
                response.raise_for_status()
                
                # –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º
                with zipfile.ZipFile(io.BytesIO(response.content)) as z:
                    # –û–±—ã—á–Ω–æ –≤ –∞—Ä—Ö–∏–≤–µ –æ–¥–∏–Ω CSV —Ñ–∞–π–ª
                    for filename in z.namelist():
                        if filename.endswith('.csv'):
                            # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
                            z.extract(filename, self.temp_dir)
                            return self.temp_dir / filename
                
                return None
                
            except requests.exceptions.HTTPError as e:
                if e.response.status_code != 404:
                    if attempt < retries - 1:
                        time.sleep(2)  # –ü–∞—É–∑–∞ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–æ–º
                        continue
                    print(f"‚ö†Ô∏è  HTTP –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {url}: {e}")
                return None
            except Exception as e:
                if attempt < retries - 1:
                    time.sleep(2)  # –ü–∞—É–∑–∞ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–æ–º
                    continue
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {url}: {e}")
                return None
        
        return None
    
    def merge_csv_files(self, csv_files, output_filename):
        """–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å–µ CSV —Ñ–∞–π–ª—ã –≤ –æ–¥–∏–Ω"""
        print(f"\nüîÑ –û–±—ä–µ–¥–∏–Ω—è—é {len(csv_files)} —Ñ–∞–π–ª–æ–≤...")
        
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ Binance kline data
        columns = [
            'open_time', 'open', 'high', 'low', 'close', 'volume',
            'close_time', 'quote_volume', 'trades', 'taker_buy_base',
            'taker_buy_quote', 'ignore'
        ]
        
        dfs = []
        for csv_file in tqdm(csv_files, desc="–ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤"):
            try:
                df = pd.read_csv(csv_file, names=columns, header=None)
                dfs.append(df)
            except Exception as e:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {csv_file}: {e}")
        
        if not dfs:
            print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è")
            return None
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ
        merged_df = pd.concat(dfs, ignore_index=True)
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –æ—á–∏—Å—Ç–∫–∞ timestamp'–æ–≤
        print("üîç –ü—Ä–æ–≤–µ—Ä—è—é –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å timestamp'–æ–≤...")
        initial_rows = len(merged_df)
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ numeric, –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ —Å—Ç–∞–Ω—É—Ç NaN
        merged_df['open_time'] = pd.to_numeric(merged_df['open_time'], errors='coerce')
        
        # –†–∞–∑—É–º–Ω—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:
        # Min: 2009-01-01 (–Ω–∞—á–∞–ª–æ Bitcoin) = 1230768000000 ms
        # Max: —Ç–µ–∫—É—â–∞—è –¥–∞—Ç–∞ + 1 –≥–æ–¥ –¥–ª—è –∑–∞–ø–∞—Å–∞
        min_timestamp = 1230768000000  # 2009-01-01
        max_timestamp = int((datetime.now() + timedelta(days=365)).timestamp() * 1000)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ timestamp'—ã
        valid_mask = (
            merged_df['open_time'].notna() & 
            (merged_df['open_time'] >= min_timestamp) & 
            (merged_df['open_time'] <= max_timestamp)
        )
        
        invalid_count = (~valid_mask).sum()
        if invalid_count > 0:
            print(f"‚ö†Ô∏è  –ù–∞–π–¥–µ–Ω–æ {invalid_count} –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö timestamp'–æ–≤, —É–¥–∞–ª—è—é...")
        
        merged_df = merged_df[valid_mask].copy()
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        merged_df = merged_df.sort_values('open_time').reset_index(drop=True)
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        before_dedup = len(merged_df)
        merged_df = merged_df.drop_duplicates(subset='open_time', keep='first')
        duplicates_removed = before_dedup - len(merged_df)
        if duplicates_removed > 0:
            print(f"üîÑ –£–¥–∞–ª–µ–Ω–æ {duplicates_removed} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º timestamp –≤ datetime
        merged_df['datetime'] = pd.to_datetime(merged_df['open_time'], unit='ms', utc=True)
        
        # –ü–µ—Ä–µ—Å—Ç–∞–≤–ª—è–µ–º –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞
        cols = ['datetime', 'open_time'] + [col for col in merged_df.columns if col not in ['datetime', 'open_time']]
        merged_df = merged_df[cols]
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        merged_df.to_csv(output_filename, index=False)
        
        print(f"\n‚úÖ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_filename}")
        print(f"üìà –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(merged_df):,} (–æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {initial_rows - len(merged_df)} –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö)")
        print(f"üìÖ –ü–µ—Ä–∏–æ–¥: {merged_df['datetime'].min()} - {merged_df['datetime'].max()}")
        
        return merged_df
    
    def cleanup(self):
        """–£–¥–∞–ª—è–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã"""
        import shutil
        if self.temp_dir.exists():
            shutil.rmtree(self.temp_dir)
            print(f"üßπ –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã —É–¥–∞–ª–µ–Ω—ã")
    
    def collect(self, output_filename):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥: —Å–∫–∞—á–∏–≤–∞–µ—Ç, —Ä–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ—Ç –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ"""
        print(f"\n{'='*60}")
        print(f"üöÄ –ù–∞—á–∏–Ω–∞—é —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {self.symbol}")
        print(f"{'='*60}")
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –∞—Ä—Ö–∏–≤–æ–≤
        zip_links = self.get_zip_links()
        
        if not zip_links:
            print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏")
            return None
        
        # –°–∫–∞—á–∏–≤–∞–µ–º –∏ —Ä–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º
        csv_files = []
        failed_urls = []
        print(f"\n‚¨áÔ∏è  –°–∫–∞—á–∏–≤–∞—é –∏ —Ä–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞—é –∞—Ä—Ö–∏–≤—ã...")
        
        successful = 0
        failed = 0
        
        for url in tqdm(zip_links, desc="–ó–∞–≥—Ä—É–∑–∫–∞"):
            csv_file = self.download_and_extract(url)
            if csv_file:
                csv_files.append(csv_file)
                successful += 1
            else:
                failed += 1
                failed_urls.append(url)
            time.sleep(0.1)  # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
        
        print(f"\nüìä –£—Å–ø–µ—à–Ω–æ: {successful}, –ü—Ä–æ–ø—É—â–µ–Ω–æ: {failed}")
        
        if failed_urls and failed <= 5:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ
            print("\n‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å:")
            for url in failed_urls:
                print(f"   - {url.split('/')[-1]}")
        
        if not csv_files:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª—ã")
            return None
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º
        df = self.merge_csv_files(csv_files, output_filename)
        
        # –û—á–∏—â–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
        self.cleanup()
        
        return df


def main():
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö BTC –∏ ETH
    """
    print("="*60)
    print("üìä BINANCE DATA COLLECTOR")
    print("üì¶ –°–±–æ—Ä 4h klines –¥–ª—è BTC –∏ ETH")
    print("="*60)
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏
    pairs = [
        {
            'symbol': 'BTCUSDT',
            'output': 'BTCUSDT_4h_full.csv'
        },
        {
            'symbol': 'ETHUSDT',
            'output': 'ETHUSDT_4h_full.csv'
        }
    ]
    
    results = {}
    
    for pair in pairs:
        collector = BinanceDataCollector(
            symbol=pair['symbol'],
            interval='4h'
        )
        
        df = collector.collect(pair['output'])
        results[pair['symbol']] = df
        
        print("\n" + "="*60 + "\n")
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print("="*60)
    print("üéâ –°–ë–û–† –î–ê–ù–ù–´–• –ó–ê–í–ï–†–®–ï–ù!")
    print("="*60)
    
    for symbol, df in results.items():
        if df is not None:
            print(f"\n{symbol}:")
            print(f"  üìÑ –§–∞–π–ª: {symbol}_4h_full.csv")
            print(f"  üìä –ó–∞–ø–∏—Å–µ–π: {len(df):,}")
            print(f"  üìÖ –ü–µ—Ä–∏–æ–¥: {df['datetime'].min()} - {df['datetime'].max()}")
            print(f"  üíæ –†–∞–∑–º–µ—Ä: ~{df.memory_usage(deep=True).sum() / 1024**2:.1f} MB –≤ –ø–∞–º—è—Ç–∏")
    
    print("\n‚ú® –ì–æ—Ç–æ–≤–æ! –ú–æ–∂–Ω–æ –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç—å –∫ EDA –∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—é —Ñ–∏—á–µ–π.")


if __name__ == "__main__":
    main()
